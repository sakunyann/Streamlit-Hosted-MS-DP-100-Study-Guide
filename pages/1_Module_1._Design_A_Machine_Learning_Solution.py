import streamlit as st

st.set_page_config(page_title="DP-100T01-A - Module 1",
                   page_icon=":memo:",
                   layout="wide")


st.markdown("## Module 1. Design A Machine Learning Solution")
st.markdown("***")
st.markdown("""
### **1.1 Design a data ingestion strategy for machine learning projects**
***33 min - Module - 7 Units***

"""
)


with st.expander("Unit 1/7 - Introduction"):
    st.markdown(
        """
    ### Learning objectives
    #### In this module, you'll learn how to:

    > - Identify your data source and format.
    > - Choose how to serve data to machine learning workflows.
    > - Design a data ingestion solution.

     ##### Sequence of events: 
    >Identify data source :arrow_right: Original data format :arrow_right: Desired data format :arrow_right: How to serve data :arrow_right: Data ingestion pipleine

    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/1-introduction?ns-enrollment-type=learningpath&ns-enrollment-id=learn.wwl.design-machine-learning-solution)
    """
    )


with st.expander("Unit 2/7 - Identify your data source and format"):
    st.markdown(
        """
    ### Six steps to plan, train, deploy, and monitor the model:

    > 1. **Define the problem:** Decide on what the model should predict and when it's successful.
    > 2. **Get the data:** Find data sources and get access.
    > 3. **Prepare the data:** Explore the data. Clean and transform the data based on the model's requirements.
    > 4. **Train the model:** Choose an algorithm and hyperparameter values based on trial and error.
    > 5. **Integrate the model:** Deploy the model to an endpoint to generate predictions.
    > 6. **Monitor the model:** Track the model's performance.

    > - **To get & prepare data to train machine learning model:** Extract data from a source & make it available to the Azure service of choice to train models/make predictions.

    #### Best Practice: Extract data from its source before analyzing it.

    > **ETL/ELT** 
    : **Extract** the data from its source, **transform** it, and **load** it into a serving layer.

    ---

    ### Identify the data source

    > - First, identify **where the data you want to use is stored.**
    > - It could be in a **database** (CRM system, SQL database) or **generated by an application** (Internet of Things (IoT) device).
    > - If no access to the data, collect new data, acquire new data from public datasets, or buy curated datasets.

    ---

    ### Identify the data format
    | Data Format           | Fields/Properties | Representation | Example |
    |-----------------------|-------------------|----------------|---------|
    | **Tablular/Structured**   | All the same, defined in schema  | 1+ **tables** where columns = features, rows = data points | **Excel, CSV** files |
    | **Semi-structured**       | Not always the same | Collection of **key-value** pairs | **JSON** object generated by real-time apps like **(IoT)** devices |
    | **Unstructured**          | No structure or schema. Data can't be queried. Need to specify how to read files. | **Files** | **Documents, images, audio, and video** files|

    ---

    ### Identify the desired data format
    > - Sometimes extracted data needs to be transformed to be more suitable for model training.
    > ##### Example: You want to train a forecasting model to perform predictive maintenance on a machine.
    > To create a dataset you can use to train the forecasting model, you may:
    > 1. Extract data measurements as JSON objects from the IoT devices.
    > 2. Convert the JSON objects to a table.
    > 3. Transform the data to get the temperature per machine per minute.


    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/2-identify-your-data-source-format)
    """
    )


with st.expander("Unit 3/7 - Choose how to serve data to machine learning workflows"):
    st.markdown(
        """
    > ### **Tip:** Store data separately from compute to *minimize costs* and *be more flexible.*

    ---

    ### Separate compute from storage

    > **Benefits of cloud:**
    > - Scale compute up or down according to demands
    > - Shut down compute and restart it according to usage

    > #### Best Practice: Store data in one tool, separate from another tool used to train models.
    > - Ensures data isn't lost and is still accessible for activities like reporting when shutting down compute

    ---

    ### Store data for model training workloads
    > - Model training solutions: **Azure Machine Learning**, **Azure Databricks**, or **Azure Synapse Analytics**
    > - 3 most common options for storing data that connect easily to above solutions:

    | Storage Solutions     | Data Format          | Features       |    Use Case     |
    |-----------------------|-------------------|----------------|---------|
    | **Azure Blob Storage** | Unstructured | Cheapest for unstructured data. Ideal for images, text, and JSON. *Can be used* for CSV. | Data scientists working with CSV files. |
    | **Azure Data Lake Storage (Gen 2)** | Unstructured | Stores files like CSV and images. Hierarchical namespace. Unlimited storage. | Giving someone access to specific file or folder. |
    | **Azure SQL Database** | Structured | Data is read as a table and schema is defined when table in database is created. | Ideal for data that doesn't change over time. |

    Note: You can also checkout [other Azure data stores](https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-decision-tree)

    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/3-choose-how-serve-data-workflows)

    """
    )



with st.expander("Unit 4/7 - Design a data ingestion solution"):
    st.markdown(
        """
    > **Data ingestion pipeline**
    > : A sequence of tasks used to move and transmform data.

    > - Tasks can be triggered manually or scheduled through the pipeline for automation.

    ---

    ### Create a data ingestion pipeline

    > Choose an Azure service to use:

    | Pipeline Service      | Use Case | Features | Compute Power |
    |-----------------------|-------------------|----------------|----------------|
    | **Azure Synapse Analytics** (Azure Synapse Pipelines) | Prefer **Easy-to-use UI** or define pipeline in **JSON** format | Easily copy data from on source to data store using one of many standard connectors. Can use UI tool (**mapping data flow**) or language (SQL, Python, R) to add data transformation task. Can be **scheduled** to run. | Can handle **large data** transformations at scale using serverless **SQL pools, dedicated SQL pools, or Spark pools.** | 
    | **Azure Databricks** | Prefer **code-first** tool: SQL, Python, or R | Define pipelines in a **notebook**. Can be **scheduled** to run. | Uses **Spark clusters** to distribute the compute to transform **large amounts of data**|
    | **Azure Machine Learning** | Train machine learning models. Also extract, transform, and store data for model prep. | Provides **compute clusters** which automatically scale up and down. Create a pipeline with the **Designer**, or by creating a collection of scripts. Create and **schedule** pipeline to run with **on-demand compute cluster.**   | **Less scalable compute** than Azure Synapse Analytics and Azure Databricks for transformations to be distributed across compute nodes.  |
    
    ---

    ### Design a data ingestion solution

    > **Architecture**
    > : Services that are linked to represent a solution.

    > ##### Example common approach for a data ingestion solution:
    > 1. Extract raw data from its source (like a CRM system or IoT device).
    > 2. Copy and transform the data with **Azure Synapse Analytics.**
    > 3. Store the prepared data in an **Azure Blob Storage.**
    > 4. Train the model with **Azure Machine Learning.**

    > #### Best Practice: think about the architecture of a data ingestion solution **before** training your model.

    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/4-solution)

    """
    )


with st.expander("Unit 5/7 - Exercise: Design a data ingestion strategy"):
    st.markdown(
        """
    
    ### Activity: Design a data ingestion strategy
    > - Give advice on how to ingest and serve the data

    | Q No. | Question                                                      | Ask (Hint) 1                  | Ask (Hint) 2                                      | Answer                   |
    |-------|---------------------------------------------------------------|-------------------------------|---------------------------------------------------|--------------------------|
    | 1     | What type of data do we currently have?                       | Data is stored in a database. | Data is in **CSV format** with **fixed columns.** | Structured (or tabular)  |
    | 2     | Which storage solution would you recommend to store the data? | Need to give access to specific file or folder **(Hierarchical namespace).** | Prefer to store data in **CSV files.**  | Azure Data Lake |
    | 3     | Which tool would you recommend we use to move the data?       | Plan to train the machine learning model with either Azure Synapse Analytics, Azure Databricks, or Azure Machine Learning | Using **Azure Synapse Analytics** to ingest and transform and **Azure SQL Database** to store data. **Automated pipelines** | Azure Synapse Analytics |
    | 4     | Which tool should we use to anonymize the patient data?       | Prefer an **easy-to-use interface.** (Mapping data flow)  |  tool should be able to handle **large amounts of data.** | Azure Synapse Analytics |
    | 5     | Which architecture represents the proposed data ingestion solution? | Data will be stored in a **Azure Data Lake Storage.** | Data will be ingested with **Azure Synapse Analytics.** | Data :arrow_right: Azure Synapse Analytics :arrow_right: Azure Data Lake |



    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/5-exercise)
    """
    )


with st.expander("Unit 6/7 - Knowledge Check"):
    st.markdown(
        """
    1. Every minute, a JSON object is extracted from an Internet of Things (IoT) device. What is the type of data that is extracted? 
    >>> **Semi-structured**
    > :  A JSON object is considered semi-structured.

    2. When a data scientist extracts JSON objects from an IoT device, and combines all transformed data in a CSV file, which data store would be best to use? 
    >>> **Azure Data Lake Storage**
    > : You can store CSV files in a data lake without having any capacity constraints.

    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/6-knowledge-check)
    """
    )

with st.expander("Unit 7/7 - Summary"):
    st.markdown(
        """
    
    #### In this module, you've learned how to:

    > 1. Identify your data source and format.
    > 2. Choose how to serve data to machine learning workflows.
    > 3. Design a data ingestion solution.

    [Source](https://learn.microsoft.com/en-us/training/modules/design-data-ingestion-strategy-for-machine-learning-projects/7-summary)
    """
    )



st.markdown("***")
st.markdown("""
### **1.2 Design a machine learning model training solution**
***32 min - Module - 7 Units***

"""
)


with st.expander("Unit 1/7 - Introduction"):
    st.markdown(
        """

    ### Learning objectives
    >####In this module, you'll learn how to:

    > - Identify machine learning tasks.
    > - Choose a service to train a model.
    > - Choose between compute options.

    [Source](https://learn.microsoft.com/en-us/training/modules/design-machine-learning-model-training-solution/1-introduction)
    """
    )

with st.expander("Unit 2/7 - Identify machine learning tasks"):
    st.markdown(
        """
    ####  6 steps to plan, train, deploy, and monitor the model:

    > 1. Define the problem: Decide on what the model should predict and when it's successful.
    > 2. Get the data: Find data sources and get access.
    > 3. Prepare the data: Explore the data. Clean and transform the data based on the model's requirements.
    > 4. Train the model: Choose an algorithm and hyperparameter values based on trial and error.
    > 5. Integrate the model: Deploy the model to an endpoint to generate predictions.
    > 6. Monitor the model: Track the model's performance.

    [Source](https://learn.microsoft.com/en-us/training/modules/design-machine-learning-model-training-solution/2-identify-tasks)
    """
    )

with st.expander("Unit 3/7 - Choose a service to train a machine learning model"):
    st.markdown(
        """

    [Source]()
    """
    )

with st.expander("Unit 4/7 - Decide between compute options"):
    st.markdown(
        """

    [Source]()
    """
    )


with st.expander("Unit 5/7 - Exercise: Design a model training strategy"):
    st.markdown(
        """
    

    ### Activity: Design a model training strategy
    > - Give advice on how to detect diabetes

    | Q No. | Question                                                      | Ask (Hint) 1                  | Ask (Hint) 2                                      | Answer                   |
    |-------|---------------------------------------------------------------|-------------------------------|---------------------------------------------------|--------------------------|
    | 1     | How should we train the model to predict diabetes?            | Train classification model using Python. Not using SQL or Spark. | Prefer **notebooks and scripts,** no UI. | Train a model using scikit-learn |
    | 2     | Which tool should we use to train the diabetes model?         | Currently working in Jupyter notebooks and need a more collaborative tool. | 
    | 3     |
    | 4     |
    | 5     |


    [Source]()
    """
    )

with st.expander("Unit 6/7 - Knowledge Check"):
    st.markdown(
        """

    [Source]()
    """
    )

with st.expander("Unit 7/7 - Summary"):
    st.markdown(
        """

    [Source]()
    """
    )